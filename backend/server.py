import io
import requests 
import torch
import torch.nn as nn
from torchvision import models, transforms
from fastapi import FastAPI, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import torch.nn.functional as F
import json
import os
import sys

app = FastAPI()

# --- è¨­å®š CORS ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# 1. å®šç¾©æ¨¡å‹æ¶æ§‹ (é›™é ­é¾)
# ==========================================
class MultiHeadResNet(nn.Module):
    def __init__(self, num_cats, num_cols):
        super().__init__()
        self.backbone = models.resnet18(pretrained=False)
        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()
        self.fc_cat = nn.Linear(num_features, num_cats)
        self.fc_color = nn.Linear(num_features, num_cols)

    def forward(self, x):
        features = self.backbone(x)
        return self.fc_cat(features), self.fc_color(features)

# ==========================================
# 2. å¾ JSON åŠ è¼‰é¡åˆ¥å’Œé¡è‰²æ˜ å°„
# ==========================================
def load_class_mappings():
    """å¾ JSON æ–‡ä»¶åŠ è¼‰é¡åˆ¥æ˜ å°„"""
    json_path = os.path.join(os.path.dirname(__file__), "class_mapping.json")
    
    if not os.path.exists(json_path):
        print(f"âŒ class_mapping.json ä¸å­˜åœ¨: {json_path}")
        print("è«‹å…ˆé‹è¡Œ train_model.py ç”Ÿæˆæ˜ å°„æ–‡ä»¶")
        sys.exit(1)
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # è½‰æ›ç‚ºå­—å…¸å½¢å¼ (JSON ä¼šæŠŠæ•´æ•°éµè½‰æˆå­—ç¬¦ä¸²)
    cat_map = {int(k): v for k, v in data['cat_map'].items()}
    color_map = {int(k): v for k, v in data['color_map'].items()}
    
    return cat_map, color_map

# åŠ è¼‰æ˜ å°„
print("æ­£åœ¨åŠ è¼‰é¡åˆ¥æ˜ å°„...")
cat_map, color_map = load_class_mappings()
CLASS_NAMES = [cat_map[i] for i in sorted(cat_map.keys())]
COLOR_NAMES = [color_map[i] for i in sorted(color_map.keys())]
NUM_CATS = len(CLASS_NAMES)
NUM_COLORS = len(COLOR_NAMES)
print(f"âœ… å·²åŠ è¼‰ {NUM_CATS} ç¨®æœè£é¡åˆ¥å’Œ {NUM_COLORS} ç¨®é¡è‰²")

# è¼‰å…¥åˆ†é¡æ¨¡å‹
classifier = None
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

try:
    print(f"æ­£åœ¨è¼‰å…¥åˆ†é¡æ¨¡å‹... (é¡åˆ¥: {NUM_CATS})")
    model = MultiHeadResNet(num_cats=NUM_CATS, num_cols=NUM_COLORS)
    # é€™è£¡è¨˜å¾—ç¢ºèª Model_Weights.pth ç¢ºå¯¦åœ¨ backend è³‡æ–™å¤¾è£¡
    state_dict = torch.load("Model_Weights.pth", map_location=device)
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()
    classifier = model
    print("âœ… Model_Weights.pth è¼‰å…¥æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ åˆ†é¡æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    print("ğŸ’¡ è«‹ç¢ºèª 'Model_Weights.pth' æ˜¯å¦å·²è¤‡è£½åˆ° backend è³‡æ–™å¤¾ä¸­")

# é è™•ç†
transform_classify = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# è¼‰å…¥ CLIP æ¨¡å‹
print("æ­£åœ¨è¼‰å…¥ CLIP æ¨¡å‹...")
CLIP_MODEL_NAME = "openai/clip-vit-base-patch32"
clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME)
clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)
print("âœ… CLIP æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")

# ==========================================
# 3. API å€åŸŸ
# ==========================================

@app.get("/")
def home():
    return {"message": "AI Backend is Running!"}

# åŠŸèƒ½ä¸€ï¼šè¾¨è­˜è¡£æœç¨®é¡èˆ‡é¡è‰²
@app.post("/predict_type")
async def predict_type(file: UploadFile = File(...)):
    if classifier is None:
        return {"category": "unknown", "color": "unknown", "error": "Model not loaded"}
    
    image_data = await file.read()
    image = Image.open(io.BytesIO(image_data)).convert("RGB")
    
    # é è™•ç†
    img_tensor = transform_classify(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        cat_logits, col_logits = classifier(img_tensor)
        
        _, cat_idx = torch.max(cat_logits, 1)
        _, col_idx = torch.max(col_logits, 1)
        
        # é˜²æ­¢ index out of range
        pred_cat = CLASS_NAMES[cat_idx.item()] if cat_idx.item() < len(CLASS_NAMES) else "unknown"
        pred_col = COLOR_NAMES[col_idx.item()] if col_idx.item() < len(COLOR_NAMES) else "unknown"

    return {"category": pred_cat, "color": pred_col}

# ğŸ”¥ åŠŸèƒ½äºŒï¼šç›´æ¥æ¥æ”¶ç¶²å€é€²è¡Œæ¯”å°
@app.post("/compare_url")
async def compare_url(file1: UploadFile = File(...), url2: str = Form(...)):
    try:
        # 1. è®€å–ä½¿ç”¨è€…ä¸Šå‚³çš„åœ–
        img1_data = await file1.read()
        img1 = Image.open(io.BytesIO(img1_data)).convert("RGB")
        
        # 2. è®“å¾Œç«¯å»ä¸‹è¼‰è¡£æ«ƒçš„åœ– (è§£æ±º CORS å•é¡Œ)
        # print(f"æ­£åœ¨ä¸‹è¼‰åœ–ç‰‡: {url2}") # é™¤éŒ¯ç”¨
        headers = {'User-Agent': 'Mozilla/5.0'} # å½è£æˆç€è¦½å™¨ï¼Œé¿å…è¢«æ“‹
        resp = requests.get(url2, headers=headers, timeout=10)
        
        if resp.status_code != 200:
            print(f"ä¸‹è¼‰å¤±æ•—: {resp.status_code} - {url2}")
            return {"similarity": 0, "message": "Download failed"}
            
        img2 = Image.open(io.BytesIO(resp.content)).convert("RGB")
        
        # 3. CLIP æ¯”å°
        inputs1 = clip_processor(images=img1, return_tensors="pt")
        inputs2 = clip_processor(images=img2, return_tensors="pt")
        
        with torch.no_grad():
            feat1 = clip_model.get_image_features(**inputs1)
            feat2 = clip_model.get_image_features(**inputs2)
            
        feat1 = feat1 / feat1.norm(p=2, dim=-1, keepdim=True)
        feat2 = feat2 / feat2.norm(p=2, dim=-1, keepdim=True)
        
        score = F.cosine_similarity(feat1, feat2).item() * 100
        return {"similarity": score, "message": "success"}
        
    except Exception as e:
        print(f"æ¯”å°éŒ¯èª¤: {e}")
        return {"similarity": 0, "message": str(e)}